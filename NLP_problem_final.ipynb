{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import numpy as np\n",
    "from utils import tokenizer\n",
    "from functools import reduce\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import utils2\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'NLP_Data/train.csv'\n",
    "test_path = 'NLP_Data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'location', 'date', 'job_title', 'summary', 'positives',\n",
       "       'negatives', 'advice_to_mgmt', 'score_1', 'score_2', 'score_3',\n",
       "       'score_4', 'score_5', 'score_6', 'overall', 'status_Current Employee ',\n",
       "       'status_Former Employee ', 'Place_startup_1', 'Place_startup_2',\n",
       "       'Place_startup_3', 'Place_startup_4', 'Place_startup_5',\n",
       "       'Place_startup_6', 'combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_features = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5']\n",
    "target = 'overall'\n",
    "df = pd.read_csv(train_path)\n",
    "df[rating_features] = df[rating_features].apply(lambda x: x.fillna(x.mean()))\n",
    "df = pd.get_dummies(df, columns = [\"status\", \"Place\"])\n",
    "df[[\"negatives\", \"positives\", \"summary\"]].fillna(value = \"\", inplace = True)\n",
    "df[\"combined\"] = df[[\"negatives\", \"positives\",\"summary\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "rating_features.extend(['status_Current Employee ',\n",
    "       'status_Former Employee ', 'Place_startup_1', 'Place_startup_2',\n",
    "       'Place_startup_3', 'Place_startup_4', 'Place_startup_5',\n",
    "       'Place_startup_6'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  score each reviews using NLTK sentiment analyzer  to use them as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"negatives\", \"positives\", \"summary\"]].fillna(value = \"\", inplace = True)\n",
    "df[\"combined\"] = df[[\"negatives\", \"positives\", \"summary\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "df[[\"negatives\", \"positives\", \"summary\"]] = df[[\"negatives\", \"positives\", \"summary\"]].astype(str)\n",
    "df[\"negatives\"] = df[\"negatives\"].apply(lambda x: utils2.tokenizer(x))\n",
    "df[\"positives\"] = df[\"positives\"].apply(lambda x: utils2.tokenizer(x))\n",
    "df[\"combined\"] = df[\"combined\"].apply(lambda x: utils2.tokenizer(x))\n",
    "\n",
    "sia = SIA()\n",
    "\n",
    "df[\"neg_score\"] = df[\"negatives\"].apply(lambda x: sia.polarity_scores(x)[\"neg\"])\n",
    "df[\"pos_score\"] = df[\"positives\"].apply(lambda x: sia.polarity_scores(x)[\"pos\"])\n",
    "rating_features.extend([\"summary_score\", 'neg_score', 'pos_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_features = ['score_1',\n",
    " 'score_2',\n",
    " 'score_3',\n",
    " 'score_4',\n",
    " 'score_5',\n",
    " 'status_Current Employee ',\n",
    " 'status_Former Employee ',\n",
    " 'Place_startup_1',\n",
    " 'Place_startup_2',\n",
    " 'Place_startup_3',\n",
    " 'Place_startup_4',\n",
    " 'Place_startup_5',\n",
    " 'Place_startup_6',\n",
    " 'summary_score',\n",
    " 'neg_score',\n",
    " 'pos_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[rating_features + [target]].corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24268, 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.2, stratify = df[target])\n",
    "df_train.dropna(subset = rating_features, inplace=True )\n",
    "df_test.dropna(subset= rating_features, inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=5, max_df = 0.7)\n",
    "feature_vec= tfidf.fit_transform(df_train[\"combined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4780x4328 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 106783 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1 = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=2, max_df = 0.9)\n",
    "tfidf_1.fit_transform(df_train[df_train[target] == 1][\"combined\"])\n",
    "\n",
    "tfidf_2 = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=2, max_df = 0.9)\n",
    "tfidf_2.fit_transform(df_train[df_train[target] == 2][\"combined\"])\n",
    "\n",
    "tfidf_3 = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=2, max_df = 0.9)\n",
    "tfidf_3.fit_transform(df_train[df_train[target] == 3][\"combined\"])\n",
    "\n",
    "tfidf_4 = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=2, max_df = 0.9)\n",
    "tfidf_4.fit_transform(df_train[df_train[target] == 4][\"combined\"])\n",
    "\n",
    "tfidf_5 = TfidfVectorizer(sublinear_tf=True, tokenizer = tokenizer, min_df=2, max_df = 0.9)\n",
    "tfidf_5.fit_transform(df_train[df_train[target] == 5][\"combined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    10688\n",
       "3.0     9510\n",
       "5.0     5975\n",
       "2.0     3531\n",
       "1.0      632\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "feature_array_1 = np.array(tfidf_1.get_feature_names())\n",
    "tfidf_sorting_1 = np.argsort(tfidf_1.idf_)[:-1]\n",
    "top_1 = set(feature_array_1[tfidf_sorting_1][:n])\n",
    "\n",
    "n = 200\n",
    "feature_array_2 = np.array(tfidf_2.get_feature_names())\n",
    "tfidf_sorting_2 = np.argsort(tfidf_2.idf_)[:-1]\n",
    "top_2 = set(feature_array_2[tfidf_sorting_2][:n])\n",
    "\n",
    "n = 200\n",
    "feature_array_3 = np.array(tfidf_3.get_feature_names())\n",
    "tfidf_sorting_3 = np.argsort(tfidf_3.idf_)[:-1]\n",
    "top_3 = set(feature_array_3[tfidf_sorting_1][:n])\n",
    "\n",
    "n = 100\n",
    "feature_array_4 = np.array(tfidf_4.get_feature_names())\n",
    "tfidf_sorting_4 = np.argsort(tfidf_4.idf_)[:-1]\n",
    "top_4 = set(feature_array_4[tfidf_sorting_1][:n])\n",
    "\n",
    "n = 200\n",
    "feature_array_5 = np.array(tfidf_5.get_feature_names())\n",
    "tfidf_sorting_5 = np.argsort(tfidf_5.idf_)[:-1]\n",
    "top_5 = set(feature_array_5[tfidf_sorting_5][:n])\n",
    "# feature_array_YN = np.array(tfidf_YN.get_feature_names())\n",
    "# tfidf_sorting_YN = np.argsort(tfidf_YN.idf_)[:-1]\n",
    "\n",
    "word_features =reduce(set.union, [top_1, top_2, top_3, top_4, top_5]) - reduce(set.intersection, [top_1, top_2, top_3, top_4, top_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = word_features.intersection(tfidf.get_feature_names())\n",
    "feature_index = [tfidf.get_feature_names().index(word) for word in list(word_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(feature_vec.toarray()[:, list(feature_index)])\n",
    "features =np.concatenate((features, np.array(df_train[rating_features].values)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "# _ = model.fit(features, df_train[target].values)\n",
    "# pred = model.predict(df_test[rating_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=50, max_depth=5, random_state=0),\n",
    "    SVC(),\n",
    "    LinearSVC(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, list(df_train[target].values), scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = tfidf.transform(df_test[\"combined\"])\n",
    "test_features = np.array(test_vec.toarray()[:, list(feature_index)])\n",
    "test_features =np.concatenate((test_features, np.array(df_test[rating_features].values)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6068, 1900)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3961766644693474"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=0)\n",
    "_ = model.fit(features, list(df_train[target].values))\n",
    "\n",
    "pred = model.predict(test_features)\n",
    "accuracy_score(df_test[target].values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1791552923543597\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_test[target].values, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |    1    2    3    4    5 |\n",
      "    |    .    .    .    .    . |\n",
      "    |    0    0    0    0    0 |\n",
      "----+--------------------------+\n",
      "1.0 |   <.>   .   97   30    . |\n",
      "2.0 |    .   <.> 450  256    . |\n",
      "3.0 |    .    . <623>1279    . |\n",
      "4.0 |    .    .  357<1781>   . |\n",
      "5.0 |    .    .   37 1158   <.>|\n",
      "----+--------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ConfusionMatrix(list(df_test[target].values), list(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>score_5</th>\n",
       "      <th>summary_score</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575368</td>\n",
       "      <td>0.466423</td>\n",
       "      <td>0.418864</td>\n",
       "      <td>0.572163</td>\n",
       "      <td>0.232032</td>\n",
       "      <td>0.613176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_2</th>\n",
       "      <td>0.575368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583594</td>\n",
       "      <td>0.466312</td>\n",
       "      <td>0.714954</td>\n",
       "      <td>0.286793</td>\n",
       "      <td>0.760170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_3</th>\n",
       "      <td>0.466423</td>\n",
       "      <td>0.583594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510693</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.223543</td>\n",
       "      <td>0.692272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_4</th>\n",
       "      <td>0.418864</td>\n",
       "      <td>0.466312</td>\n",
       "      <td>0.510693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.476415</td>\n",
       "      <td>0.227571</td>\n",
       "      <td>0.542776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_5</th>\n",
       "      <td>0.572163</td>\n",
       "      <td>0.714954</td>\n",
       "      <td>0.629550</td>\n",
       "      <td>0.476415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>0.728527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary_score</th>\n",
       "      <td>0.232032</td>\n",
       "      <td>0.286793</td>\n",
       "      <td>0.223543</td>\n",
       "      <td>0.227571</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.268730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0.613176</td>\n",
       "      <td>0.760170</td>\n",
       "      <td>0.692272</td>\n",
       "      <td>0.542776</td>\n",
       "      <td>0.728527</td>\n",
       "      <td>0.268730</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score_1   score_2   score_3   score_4   score_5  \\\n",
       "score_1        1.000000  0.575368  0.466423  0.418864  0.572163   \n",
       "score_2        0.575368  1.000000  0.583594  0.466312  0.714954   \n",
       "score_3        0.466423  0.583594  1.000000  0.510693  0.629550   \n",
       "score_4        0.418864  0.466312  0.510693  1.000000  0.476415   \n",
       "score_5        0.572163  0.714954  0.629550  0.476415  1.000000   \n",
       "summary_score  0.232032  0.286793  0.223543  0.227571  0.245949   \n",
       "overall        0.613176  0.760170  0.692272  0.542776  0.728527   \n",
       "\n",
       "               summary_score   overall  \n",
       "score_1             0.232032  0.613176  \n",
       "score_2             0.286793  0.760170  \n",
       "score_3             0.223543  0.692272  \n",
       "score_4             0.227571  0.542776  \n",
       "score_5             0.245949  0.728527  \n",
       "summary_score       1.000000  0.268730  \n",
       "overall             0.268730  1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['score_1', 'score_2', 'score_3', 'score_4', 'score_5', \"summary_score\", 'overall']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_features = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5']\n",
    "target = 'overall'\n",
    "df1 = pd.read_csv(test_path)\n",
    "df1[rating_features] = df1[rating_features].apply(lambda x: x.fillna(x.mean()))\n",
    "df1 = pd.get_dummies(df1, columns = [\"status\", \"Place\"])\n",
    "df1[[\"negatives\", \"positives\", \"summary\"]].fillna(value = \"\", inplace = True)\n",
    "df1[\"combined\"] = df1[[\"negatives\", \"positives\",\"summary\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "rating_features.extend(['status_Current Employee ',\n",
    "       'status_Former Employee ', 'Place_startup_1', 'Place_startup_2',\n",
    "       'Place_startup_3', 'Place_startup_4', 'Place_startup_5',\n",
    "       'Place_startup_6'])\n",
    "df1.columns\n",
    "\n",
    "\n",
    "df1[[\"negatives\", \"positives\", \"summary\"]].fillna(value = \"\", inplace = True)\n",
    "df1[\"combined\"] = df1[[\"negatives\", \"positives\", \"summary\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "df1[[\"negatives\", \"positives\", \"summary\"]] = df1[[\"negatives\", \"positives\", \"summary\"]].astype(str)\n",
    "df1[\"negatives\"] = df1[\"negatives\"].apply(lambda x: utils2.tokenizer(x))\n",
    "df1[\"positives\"] = df1[\"positives\"].apply(lambda x: utils2.tokenizer(x))\n",
    "df1[\"combined\"] = df1[\"combined\"].apply(lambda x: utils2.tokenizer(x))\n",
    "\n",
    "sia = SIA()\n",
    "\n",
    "df1[\"summary_score\"] = df1[\"combined\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "df1[\"neg_score\"] = df1[\"negatives\"].apply(lambda x: sia.polarity_scores(x)[\"neg\"])\n",
    "df1[\"pos_score\"] = df1[\"positives\"].apply(lambda x: sia.polarity_scores(x)[\"pos\"])\n",
    "rating_features.extend([\"summary_score\", 'neg_score', 'pos_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = tfidf.transform(df1[\"combined\"])\n",
    "test_features = np.array(test_vec.toarray()[:, list(feature_index)])\n",
    "test_features =np.concatenate((test_features, np.array(df1[rating_features].values)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(df1[rating_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = {\"ID\":df1[\"ID\"].values, 'overall': predictions}\n",
    "df_pred = pd.DataFrame(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer = tokenizer, min_df=1, max_df=0.9)\n",
    "count_vec = cv.fit_transform(df_train[\"negatives\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(tokenizer = tokenizer, min_df=5, max_df = 0.90)\n",
    "count_vec1 = cv1.fit_transform(df_train[df_train[target] == 1][\"negatives\"])\n",
    "\n",
    "cv2 = CountVectorizer(tokenizer = tokenizer, min_df=5, max_df = 0.90)\n",
    "count_vec2 = cv2.fit_transform(df_train[df_train[target] == 2][\"negatives\"])\n",
    "\n",
    "cv3 = CountVectorizer(tokenizer = tokenizer, min_df=5, max_df = 0.90)\n",
    "count_vec3 = cv3.fit_transform(df_train[df_train[target] == 3][\"negatives\"])\n",
    "\n",
    "cv4 = CountVectorizer(tokenizer = tokenizer, min_df=5, max_df = 0.90)\n",
    "count_vec4 =  cv4.fit_transform(df_train[df_train[target] == 4][\"negatives\"])\n",
    "\n",
    "cv5 = CountVectorizer(tokenizer = tokenizer, min_df=5, max_df = 0.90)\n",
    "count_vec5 = cv5.fit_transform(df_train[df_train[target] == 5][\"negatives\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1 = {word:word_count  for word, word_count in  zip(cv1.get_feature_names(),count_vec1.toarray().sum(axis=0)) }\n",
    "words_1 = dict(sorted(words_1.items(), key = lambda x: x[1], reverse=True)[:100])\n",
    "words_1 = set([x[0] for x in words_1.items()])\n",
    "\n",
    "words_2 = {word:word_count  for word, word_count in  zip(cv2.get_feature_names(),count_vec2.toarray().sum(axis=0)) }\n",
    "words_2 = dict(sorted(words_2.items(), key = lambda x: x[1], reverse=True)[:100])\n",
    "words_2 = set([x[0] for x in words_2.items()])\n",
    "\n",
    "words_3 = {word:word_count  for word, word_count in  zip(cv1.get_feature_names(),count_vec3.toarray().sum(axis=0)) }\n",
    "words_3 = dict(sorted(words_3.items(), key = lambda x: x[1], reverse=True)[:10])\n",
    "words_3 = set([x[0] for x in words_3.items()])\n",
    "\n",
    "words_4 = {word:word_count  for word, word_count in  zip(cv4.get_feature_names(),count_vec4.toarray().sum(axis=0)) }\n",
    "words_4 = dict(sorted(words_4.items(), key = lambda x: x[1], reverse=True)[:100])\n",
    "words_4 = set([x[0] for x in words_4.items()])\n",
    "\n",
    "\n",
    "words_5 = {word:word_count  for word, word_count in  zip(cv5.get_feature_names(),count_vec5.toarray().sum(axis=0)) }\n",
    "words_5 = dict(sorted(words_5.items(), key = lambda x: x[1], reverse=True)[:100])\n",
    "words_5 = set([x[0] for x in words_5.items()])\n",
    "\n",
    "top_words = [words_1, words_2,words_3,words_4,words_5]\n",
    "\n",
    "word_features1 = reduce(set.union, top_words) - reduce(set.intersection, top_words)\n",
    "\n",
    "# from itertools import combinations \n",
    "# for words1, words2 in combinations(top_words, 2):\n",
    "#     word_features1 = reduce(set.union, [word_features1]) - reduce(set.intersection, [words1, words2])\n",
    "# word_features1 = reduce(set.union, top_words) - reduce(set.intersection, [words_5, words_4])\n",
    "# word_features1 = reduce(set.union, top_words) - reduce(set.intersection, [words_3, words_4])\n",
    "# word_features1 = reduce(set.union, top_words) - reduce(set.intersection, [words_2, words_4])\n",
    "# word_features1 = reduce(set.union, top_words) - reduce(set.intersection, [words_1, words_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(corpus, features, labels = None):\n",
    "        ###\n",
    "        if not(labels is None):\n",
    "            corpus = [tokenizer(sentence) for sentence, label in zip(corpus, labels)]\n",
    "            feature_set = [({token: token in tokens for token in features}, label) for tokens, label in zip(corpus, labels)]\n",
    "        else:\n",
    "            corpus = [tokenizer(sentence) for sentence in corpus]\n",
    "            feature_set = [{token: token in tokens for token in features} for tokens in corpus]\n",
    "        return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = extract_features(df_train[\"negatives\"], word_features1, labels = df_train[target])\n",
    "classifier = NaiveBayesClassifier.train(feature_set)\n",
    "df_test.dropna(subset=[\"negatives\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |    1    2    3    4    5 |\n",
      "    |    .    .    .    .    . |\n",
      "    |    0    0    0    0    0 |\n",
      "----+--------------------------+\n",
      "1.0 |   <2>   7   84   30    4 |\n",
      "2.0 |   11  <24> 405  241   25 |\n",
      "3.0 |   15   19 <945> 816  107 |\n",
      "4.0 |   14   12  938<1003> 171 |\n",
      "5.0 |    3    8  477  579 <128>|\n",
      "----+--------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "0.34640738299274887\n"
     ]
    }
   ],
   "source": [
    "test_features = extract_features(df_test[\"negatives\"].values, word_features1)\n",
    "predictions = classifier.classify_many(test_features)\n",
    "print(ConfusionMatrix(list(df_test[target].values), list(predictions)))\n",
    "print(accuracy_score(df_test[target].values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21145715566128115\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(df_test[target].values, predictions, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
